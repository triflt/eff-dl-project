{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8686ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ab404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from lstm import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3df464",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "DATA_DIR = \"./data\"\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MIN_FREQ = 2\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd7e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sms_dataset() -> list[tuple[int, str]]:\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    zip_path = os.path.join(DATA_DIR, \"smsspam.zip\")\n",
    "    txt_path = os.path.join(DATA_DIR, \"SMSSpamCollection\")\n",
    "\n",
    "    if not os.path.exists(txt_path):\n",
    "        print(\"Downloading dataset...\")\n",
    "        r = requests.get(DATA_URL, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        with zipfile.ZipFile(zip_path) as zf:\n",
    "            zf.extractall(DATA_DIR)\n",
    "    else:\n",
    "        print(\"Dataset already present.\")\n",
    "\n",
    "    data = []\n",
    "    with io.open(txt_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            label, text = line.strip().split(\"\\t\", 1)\n",
    "            y = 1 if label == \"spam\" else 0\n",
    "            data.append((y, text))\n",
    "    return data\n",
    "\n",
    "# ----------------------------\n",
    "# Tokenization & vocab\n",
    "# ----------------------------\n",
    "TOKEN_RE = re.compile(r\"\\b\\w+\\b\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def tokenize(s: str) -> list[str]:\n",
    "    return TOKEN_RE.findall(s.lower())\n",
    "\n",
    "\n",
    "PAD, UNK = \"<pad>\", \"<unk>\"\n",
    "\n",
    "\n",
    "def build_vocab(texts: list[list[str]]):\n",
    "    counter = Counter()\n",
    "    for toks in texts:\n",
    "        counter.update(toks)\n",
    "    # keep by freq, cap by size\n",
    "    vocab = [PAD, UNK]\n",
    "    for w, c in counter.most_common():\n",
    "        if c < MIN_FREQ: break\n",
    "        vocab.append(w)\n",
    "        if len(vocab) >= MAX_VOCAB_SIZE: break\n",
    "    stoi = {w:i for i,w in enumerate(vocab)}\n",
    "    itos = vocab\n",
    "    return stoi, itos\n",
    "\n",
    "\n",
    "def encode(tokens: list[str], stoi: dict) -> list[int]:\n",
    "    unk = stoi.get(UNK)\n",
    "    return [stoi.get(t, unk) for t in tokens]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset with dynamic padding\n",
    "# ----------------------------\n",
    "class SMSDataset(Dataset):\n",
    "    def __init__(self, samples, stoi):\n",
    "        self.labels = [y for y, _ in samples]\n",
    "        self.texts = [encode(tokenize(x), stoi) for _, x in samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_batch(batch, pad_idx):\n",
    "    # batch: list of (tensor_ids, label)\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    max_len = lengths.max().item()\n",
    "    padded = torch.full((len(seqs), max_len), pad_idx, dtype=torch.long)\n",
    "    for i, s in enumerate(seqs):\n",
    "        padded[i, :len(s)] = s\n",
    "    # sort by length desc for pack_padded_sequence\n",
    "    lengths, sort_idx = lengths.sort(descending=True)\n",
    "    padded = padded[sort_idx]\n",
    "    labels = torch.stack(labels)[sort_idx]\n",
    "    return padded, lengths, labels\n",
    "\n",
    "\n",
    "class LSTMTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_classes: int, pad_idx: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        embedded = self.embedding(input_ids)\n",
    "        outputs, _ = self.lstm(embedded)\n",
    "        batch_range = torch.arange(outputs.size(0), device=outputs.device)\n",
    "        last_indices = lengths - 1\n",
    "        last_hidden = outputs[batch_range, last_indices]\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "    def to_qat(self, bits: int, qat_linear_class, **qat_kwargs) -> \"LSTMTextClassifier\":\n",
    "        new_model = LSTMTextClassifier(\n",
    "            vocab_size=self.embedding.num_embeddings,\n",
    "            embed_dim=self.embedding.embedding_dim,\n",
    "            hidden_dim=self.fc.in_features,\n",
    "            num_classes=self.fc.out_features,\n",
    "            pad_idx=self.embedding.padding_idx,\n",
    "        )\n",
    "        new_model.lstm = self.lstm.to_qat(bits, qat_linear_class, **qat_kwargs)\n",
    "        return new_model.to(self.fc.weight.device)\n",
    "\n",
    "    def quantize(self, bits: int, linear_int_class) -> \"LSTMTextClassifier\":\n",
    "        new_model = LSTMTextClassifier(\n",
    "            vocab_size=self.embedding.num_embeddings,\n",
    "            embed_dim=self.embedding.embedding_dim,\n",
    "            hidden_dim=self.fc.in_features,\n",
    "            num_classes=self.fc.out_features,\n",
    "            pad_idx=self.embedding.padding_idx,\n",
    "        )\n",
    "        new_model.lstm = self.lstm.quantize(bits, linear_int_class)\n",
    "        return new_model.to(self.fc.weight.device)\n",
    "\n",
    "\n",
    "def accuracy(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    correct = (predictions == targets).sum().item()\n",
    "    return correct / targets.size(0)\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, lengths, labels in dataloader:\n",
    "        inputs, lengths, labels = inputs.to(device), lengths.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs, lengths)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    return epoch_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    for inputs, lengths, labels in dataloader:\n",
    "        inputs, lengths, labels = inputs.to(device), lengths.to(device), labels.to(device)\n",
    "        logits = model(inputs, lengths)\n",
    "        loss = criterion(logits, labels)\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_acc += accuracy(logits, labels) * inputs.size(0)\n",
    "\n",
    "    dataset_size = len(dataloader.dataset)\n",
    "    return epoch_loss / dataset_size, epoch_acc / dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25154f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already present.\n",
      "Vocab size: 4349\n"
     ]
    }
   ],
   "source": [
    "data = download_sms_dataset()\n",
    "# Tokenize once to build vocab on train only\n",
    "train_p = 0.7\n",
    "train_set, test_set = random_split(data, lengths=(train_p, 1 - train_p))\n",
    "\n",
    "all_tokens = [tokenize(txt) for _, txt in data]\n",
    "stoi, itos = build_vocab(all_tokens)\n",
    "pad_idx = stoi[PAD]\n",
    "vocab_size = len(itos)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# Datasets\n",
    "ds_train = SMSDataset(train_set, stoi)\n",
    "ds_test  = SMSDataset(test_set, stoi)\n",
    "\n",
    "# Loaders\n",
    "collate = lambda b: collate_batch(b, pad_idx)\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "dl_test  = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTMTextClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=64,\n",
    "    hidden_dim=64,\n",
    "    num_classes=2,\n",
    "    pad_idx=pad_idx,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a986208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.1447 | val_loss=0.1172 | val_acc=97.1%\n"
     ]
    }
   ],
   "source": [
    "train_loss = train_epoch(model, dl_train, criterion, optimizer, device)\n",
    "val_loss, val_acc = evaluate(model, dl_test, criterion, device)\n",
    "print(f\"Epoch {1:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558b5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsq.utils import QALinear, LinearInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "600fecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qat = model.to_qat(bits=8, qat_linear_class=QALinear)\n",
    "optimizer_qa = torch.optim.Adam(model_qat.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a1c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train_loss=0.1684 | val_loss=0.0551 | val_acc=98.4%\n",
      "Epoch 00 | val_loss=0.6582 | val_acc=64.4%\n",
      "Epoch 01 | train_loss=0.0361 | val_loss=0.0657 | val_acc=98.0%\n",
      "Epoch 01 | val_loss=0.7103 | val_acc=45.8%\n",
      "Epoch 02 | train_loss=0.0175 | val_loss=0.0700 | val_acc=98.3%\n",
      "Epoch 02 | val_loss=0.7854 | val_acc=29.5%\n",
      "Epoch 03 | train_loss=0.0025 | val_loss=0.0619 | val_acc=98.8%\n",
      "Epoch 03 | val_loss=0.9196 | val_acc=16.6%\n",
      "Epoch 04 | train_loss=0.0006 | val_loss=0.0703 | val_acc=98.7%\n",
      "Epoch 04 | val_loss=0.6319 | val_acc=73.3%\n",
      "Epoch 05 | train_loss=0.0002 | val_loss=0.0758 | val_acc=98.6%\n",
      "Epoch 05 | val_loss=0.6595 | val_acc=64.2%\n",
      "Epoch 06 | train_loss=0.0002 | val_loss=0.0804 | val_acc=98.6%\n",
      "Epoch 06 | val_loss=0.7313 | val_acc=42.1%\n",
      "Epoch 07 | train_loss=0.0001 | val_loss=0.0818 | val_acc=98.6%\n",
      "Epoch 07 | val_loss=0.6017 | val_acc=79.8%\n",
      "Epoch 08 | train_loss=0.0001 | val_loss=0.0868 | val_acc=98.5%\n",
      "Epoch 08 | val_loss=0.6380 | val_acc=74.0%\n",
      "Epoch 09 | train_loss=0.0001 | val_loss=0.0899 | val_acc=98.6%\n",
      "Epoch 09 | val_loss=0.6657 | val_acc=61.8%\n",
      "Epoch 10 | train_loss=0.0000 | val_loss=0.0935 | val_acc=98.6%\n",
      "Epoch 10 | val_loss=0.6885 | val_acc=55.4%\n",
      "Epoch 11 | train_loss=0.0000 | val_loss=0.0946 | val_acc=98.6%\n",
      "Epoch 11 | val_loss=0.8589 | val_acc=24.3%\n",
      "Epoch 12 | train_loss=0.0000 | val_loss=0.0989 | val_acc=98.6%\n",
      "Epoch 12 | val_loss=0.7823 | val_acc=30.6%\n",
      "Epoch 13 | train_loss=0.0000 | val_loss=0.1001 | val_acc=98.6%\n",
      "Epoch 13 | val_loss=0.6950 | val_acc=53.6%\n",
      "Epoch 14 | train_loss=0.0000 | val_loss=0.1025 | val_acc=98.6%\n",
      "Epoch 14 | val_loss=0.6644 | val_acc=61.5%\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    train_loss = train_epoch(model_qat, dl_train, criterion, optimizer_qa, device)\n",
    "    val_loss, val_acc = evaluate(model_qat, dl_test, criterion, device)\n",
    "    print(f\"Epoch {i:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")\n",
    "\n",
    "    model_qantized = model_qat.quantize(bits=8, linear_int_class=LinearInt)\n",
    "    model_qantized.to('cpu')\n",
    "    val_loss, val_acc = evaluate(model_qantized, dl_test, criterion, 'cpu')\n",
    "    print(f\"Epoch {i:02d} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009dcba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
