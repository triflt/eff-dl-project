{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8686ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "675ab404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from lstm import LSTMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3df464",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "DATA_DIR = \"./data\"\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MIN_FREQ = 2\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cd7e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sms_dataset() -> list[tuple[int, str]]:\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    zip_path = os.path.join(DATA_DIR, \"smsspam.zip\")\n",
    "    txt_path = os.path.join(DATA_DIR, \"SMSSpamCollection\")\n",
    "\n",
    "    if not os.path.exists(txt_path):\n",
    "        print(\"Downloading dataset...\")\n",
    "        r = requests.get(DATA_URL, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        with zipfile.ZipFile(zip_path) as zf:\n",
    "            zf.extractall(DATA_DIR)\n",
    "    else:\n",
    "        print(\"Dataset already present.\")\n",
    "\n",
    "    data = []\n",
    "    with io.open(txt_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            label, text = line.strip().split(\"\\t\", 1)\n",
    "            y = 1 if label == \"spam\" else 0\n",
    "            data.append((y, text))\n",
    "    return data\n",
    "\n",
    "TOKEN_RE = re.compile(r\"\\b\\w+\\b\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def tokenize(s: str) -> list[str]:\n",
    "    return TOKEN_RE.findall(s.lower())\n",
    "\n",
    "\n",
    "PAD, UNK = \"<pad>\", \"<unk>\"\n",
    "\n",
    "\n",
    "def build_vocab(texts: list[list[str]]):\n",
    "    counter = Counter()\n",
    "    for toks in texts:\n",
    "        counter.update(toks)\n",
    "    # keep by freq, cap by size\n",
    "    vocab = [PAD, UNK]\n",
    "    for w, c in counter.most_common():\n",
    "        if c < MIN_FREQ: break\n",
    "        vocab.append(w)\n",
    "        if len(vocab) >= MAX_VOCAB_SIZE: break\n",
    "    stoi = {w:i for i,w in enumerate(vocab)}\n",
    "    itos = vocab\n",
    "    return stoi, itos\n",
    "\n",
    "\n",
    "def encode(tokens: list[str], stoi: dict) -> list[int]:\n",
    "    unk = stoi.get(UNK)\n",
    "    return [stoi.get(t, unk) for t in tokens]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset with dynamic padding\n",
    "# ----------------------------\n",
    "class SMSDataset(Dataset):\n",
    "    def __init__(self, samples, stoi):\n",
    "        self.labels = [y for y, _ in samples]\n",
    "        self.texts = [encode(tokenize(x), stoi) for _, x in samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_batch(batch, pad_idx):\n",
    "    # batch: list of (tensor_ids, label)\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    max_len = lengths.max().item()\n",
    "    padded = torch.full((len(seqs), max_len), pad_idx, dtype=torch.long)\n",
    "    for i, s in enumerate(seqs):\n",
    "        padded[i, :len(s)] = s\n",
    "    # sort by length desc for pack_padded_sequence\n",
    "    lengths, sort_idx = lengths.sort(descending=True)\n",
    "    padded = padded[sort_idx]\n",
    "    labels = torch.stack(labels)[sort_idx]\n",
    "    return padded, lengths, labels\n",
    "\n",
    "def accuracy(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    correct = (predictions == targets).sum().item()\n",
    "    return correct / targets.size(0)\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, lengths, labels in dataloader:\n",
    "        inputs, lengths, labels = inputs.to(device), lengths.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs, lengths)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    return epoch_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    for inputs, lengths, labels in dataloader:\n",
    "        if inputs.shape[0] < 16:\n",
    "            continue\n",
    "        inputs, lengths, labels = inputs.to(device), lengths.to(device), labels.to(device)\n",
    "        logits = model(inputs, lengths)\n",
    "        loss = criterion(logits, labels)\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_acc += accuracy(logits, labels) * inputs.size(0)\n",
    "\n",
    "    dataset_size = len(dataloader.dataset)\n",
    "    return epoch_loss / dataset_size, epoch_acc / dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25154f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already present.\n",
      "Vocab size: 4349\n"
     ]
    }
   ],
   "source": [
    "data = download_sms_dataset()\n",
    "# Tokenize once to build vocab on train only\n",
    "train_p = 0.7\n",
    "train_set, test_set = random_split(data, lengths=(train_p, 1 - train_p))\n",
    "\n",
    "all_tokens = [tokenize(txt) for _, txt in data]\n",
    "stoi, itos = build_vocab(all_tokens)\n",
    "pad_idx = stoi[PAD]\n",
    "vocab_size = len(itos)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# Datasets\n",
    "ds_train = SMSDataset(train_set, stoi)\n",
    "ds_test  = SMSDataset(test_set, stoi)\n",
    "\n",
    "# Loaders\n",
    "collate = lambda b: collate_batch(b, pad_idx)\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "dl_test  = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=64,\n",
    "    hidden_dim=64,\n",
    "    num_classes=2,\n",
    "    pad_idx=pad_idx,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a986208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.1425 | val_loss=0.1024 | val_acc=98.0%\n"
     ]
    }
   ],
   "source": [
    "train_loss = train_epoch(model, dl_train, criterion, optimizer, device)\n",
    "val_loss, val_acc = evaluate(model, dl_test, criterion, device)\n",
    "print(f\"Epoch {1:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9d78e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaround.utils import AdaRoundLinear, LinearInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bdf47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qat = model.to_qat(bits=8, qat_linear_class=AdaRoundLinear)\n",
    "optimizer_qa = torch.optim.Adam(model_qat.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5273623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train_loss=0.3957 | val_loss=0.3821 | val_acc=85.9%\n",
      "Epoch 00 | val_loss=0.5537 | val_acc=85.9%\n",
      "Epoch 01 | train_loss=0.3084 | val_loss=0.2786 | val_acc=85.9%\n",
      "Epoch 01 | val_loss=0.4946 | val_acc=85.9%\n",
      "Epoch 02 | train_loss=0.1139 | val_loss=0.0738 | val_acc=98.1%\n",
      "Epoch 02 | val_loss=0.4833 | val_acc=85.9%\n",
      "Epoch 03 | train_loss=0.0498 | val_loss=0.0685 | val_acc=97.4%\n",
      "Epoch 03 | val_loss=0.4801 | val_acc=85.9%\n",
      "Epoch 04 | train_loss=0.0413 | val_loss=0.0602 | val_acc=97.8%\n",
      "Epoch 04 | val_loss=0.4756 | val_acc=85.9%\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    train_loss = train_epoch(model_qat, dl_train, criterion, optimizer_qa, device)\n",
    "    val_loss, val_acc = evaluate(model_qat, dl_test, criterion, device)\n",
    "    print(f\"Epoch {i:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")\n",
    "\n",
    "    model_qantized = model_qat.quantize(bits=8, linear_int_class=LinearInt)\n",
    "    model_qantized.to('cpu')\n",
    "    val_loss, val_acc = evaluate(model_qantized, dl_test, criterion, 'cpu')\n",
    "    print(f\"Epoch {i:02d} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf334053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.1024 | val_acc=98.0%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = evaluate(model, dl_test, criterion, device)\n",
    "print(f\"val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e13910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pact.utils import QuantLinear, LinearInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e15421af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qat = model.to_qat(bits=8, qat_linear_class=QuantLinear)\n",
    "optimizer_qa = torch.optim.Adam(model_qat.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3387facd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train_loss=0.4054 | val_loss=0.4170 | val_acc=85.9%\n",
      "Epoch 00 | val_loss=0.5518 | val_acc=85.9%\n",
      "Epoch 01 | train_loss=0.3984 | val_loss=0.4128 | val_acc=85.9%\n",
      "Epoch 01 | val_loss=0.5043 | val_acc=85.9%\n",
      "Epoch 02 | train_loss=0.3921 | val_loss=0.3947 | val_acc=85.9%\n",
      "Epoch 02 | val_loss=0.4618 | val_acc=85.9%\n",
      "Epoch 03 | train_loss=0.3559 | val_loss=0.3420 | val_acc=85.9%\n",
      "Epoch 03 | val_loss=0.4351 | val_acc=85.9%\n",
      "Epoch 04 | train_loss=0.3237 | val_loss=0.3210 | val_acc=85.9%\n",
      "Epoch 04 | val_loss=0.4233 | val_acc=85.9%\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    train_loss = train_epoch(model_qat, dl_train, criterion, optimizer_qa, device)\n",
    "    val_loss, val_acc = evaluate(model_qat, dl_test, criterion, device)\n",
    "    print(f\"Epoch {i:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")\n",
    "\n",
    "    model_qantized = model_qat.quantize(bits=8, linear_int_class=LinearInt)\n",
    "    model_qantized.to('cpu')\n",
    "    val_loss, val_acc = evaluate(model_qantized, dl_test, criterion, 'cpu')\n",
    "    print(f\"Epoch {i:02d} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99285e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.1024 | val_acc=98.0%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = evaluate(model, dl_test, criterion, device)\n",
    "print(f\"val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "558b5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsq.utils import QALinear, LinearInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "600fecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qat = model.to_qat(bits=8, qat_linear_class=QALinear)\n",
    "optimizer_qa = torch.optim.Adam(model_qat.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train_loss=0.0305 | val_loss=0.0773 | val_acc=98.4%\n",
      "Epoch 00 | val_loss=0.0668 | val_acc=98.0%\n",
      "Epoch 01 | train_loss=0.0120 | val_loss=0.0720 | val_acc=98.6%\n",
      "Epoch 01 | val_loss=0.0754 | val_acc=98.1%\n",
      "Epoch 02 | train_loss=0.0066 | val_loss=0.0815 | val_acc=98.3%\n",
      "Epoch 02 | val_loss=0.0841 | val_acc=98.0%\n",
      "Epoch 03 | train_loss=0.0007 | val_loss=0.0920 | val_acc=98.6%\n",
      "Epoch 03 | val_loss=0.0895 | val_acc=98.3%\n",
      "Epoch 04 | train_loss=0.0002 | val_loss=0.1088 | val_acc=98.4%\n",
      "Epoch 04 | val_loss=0.0984 | val_acc=98.4%\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    train_loss = train_epoch(model_qat, dl_train, criterion, optimizer_qa, device)\n",
    "    val_loss, val_acc = evaluate(model_qat, dl_test, criterion, device)\n",
    "    print(f\"Epoch {i:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")\n",
    "\n",
    "    model_quantized = model_qat.quantize(bits=8, linear_int_class=LinearInt)\n",
    "    val_loss, val_acc = evaluate(model_quantized, dl_test, criterion, 'cpu')\n",
    "    print(f\"Epoch {i:02d} | val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "009dcba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss=0.1024 | val_acc=98.0%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = evaluate(model, dl_test, criterion, device)\n",
    "print(f\"val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqc = model_quantized.to(\"cuda\")\n",
    "mqatc = model_qat.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29937cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 s, sys: 144 ms, total: 1.25 s\n",
      "Wall time: 847 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09842592887271254, 0.979066985645933)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(mqc, dl_test, criterion, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5f08e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 124 ms, total: 1.4 s\n",
      "Wall time: 977 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_loss, val_acc = evaluate(mqatc, dl_test, criterion, 'cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
